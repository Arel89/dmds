{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advertising Sales Data\n",
    "\n",
    "In this session, we will use the advertising sales data that is provided by one of the authors of the book [Introduction to Statsitical Learning](http://www-bcf.usc.edu/~gareth/ISL). Since the data is available as a CSV, wen can simply import the data set by using the corresponding Pandas builder function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Radio  Newspaper  Sales\n",
       "1  230.1   37.8       69.2   22.1\n",
       "2   44.5   39.3       45.1   10.4\n",
       "3   17.2   45.9       69.3    9.3\n",
       "4  151.5   41.3       58.5   18.5\n",
       "5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side topic: Matplotlib\n",
    "\n",
    "Pandas provides many convenience features for plotting. Yet, quite often, we would like to customize our plots, and cannot find an appropriate knob to turn. Pandas plotting is based on a library called [Matplotlib](http://matplotlib.org/index.html) which allows to produce 'production-quality figures'. Whenever we would like to customize our figures more than it is allowed by Pandas, we simply take the Matplotlib object that is used by Pandas' plotting and then configure the plot to our needs.\n",
    "\n",
    "Here is an example of placing two scatter plots in a (1x2)-grid to display them horizontally in addition to setting a few parameters. The method *show()* avoids that the *to_string()* method for the last line is executed, so that only the plots are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAEPCAYAAAC9cimeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe2c9f8c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fix, (ax1,ax2) = plt.subplots(1,2, figsize=(12,4))\n",
    "df.plot(kind='scatter', x='TV', y='Sales', ax=ax1, xlim=[0,300], s=5)\n",
    "df.plot(kind='scatter', x='Radio', y='Sales', ax=ax2, s=20, color='red')\n",
    "ax2.set_xlim(0,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "We would now like to fit a linear model to predict sales based on predictors 'TV', 'Radio', and 'Newspaper'. This requires us to determine a measure fore the quality of a fit and how to use this measure to search for model parameters. \n",
    "\n",
    "### Least Mean Squares\n",
    "\n",
    "Denote $y_i$, $x_i=(x_{i1},...,x_{ip})$ as the $i$-th vector of predictors and their (observed) response, with $i \\in \\{1,...,n \\}$, and $b=(b_1,\\dots,b_p)$ as vector of model parameters.\n",
    "\n",
    "Given an observation $x_i$, the corresponding prediction is \n",
    "\n",
    "$$\\hat{y_i} = b' x_i$$\n",
    "\n",
    "Unless prediction is perfect, we measure the quality of the fit, or the error, as the squared difference between prediction and response, $(y_i - b' x_i)^2$.\n",
    "\n",
    "With $Q \\to b$ as the mean of squared differences, the objective is to select parameters $b$, such $Q(b)$ is minimized. \n",
    "\n",
    "$$Q(b) = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{2} (y_i - b' x_i)^2$$\n",
    "\n",
    "A possible solution is based on gradient descent. For this, we must first obtain the partial derivatives with respect to $b_j \\in \\{b_1,\\dots,b_p \\}$,\n",
    "\n",
    "$$\\begin{align}\\frac{\\partial Q(b)}{\\partial b_j} &=  \\frac{1}{n} \\sum_{i=1}^{n} 2 \\frac{1}{2}(y_i - b_j x_{ij})\\frac{\\partial }{\\partial b_j} (y_i - b_j x_{ij})  \\\\&=  \\frac{1}{n} \\sum_{i=1}^{n} -(y_i - b_j x_{ij})x_{ij} \\\\&= \\frac{1}{n} \\sum_{i=1}^{n} (b_j x_{ij}-y_i)x_{ij}\\end{align}$$\n",
    "\n",
    "Instead of computing the point where all partial derivatives are jointly zero, we can compute the partial derivatives for a given $b$, which gives us the gradient of function $Q$ at this point,\n",
    "\n",
    "$$\\nabla Q(b) = -\\left(\\frac{\\partial Q(b)}{\\partial b_1},\\dots,\\frac{\\partial Q(b)}{\\partial b_p}\\right)$$\n",
    "\n",
    "The gradient provides us with the direction of steepest descent towards an optimal vector of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Advertising Model\n",
    "\n",
    "Our basic sales advertising model assumes a linear relationship between sales and investments in TV and radio ads:\n",
    "    \n",
    "$$\\text{sales} = b_0 + b_1 \\times \\text{TV} +  b_2 \\times \\text{radio}$$\n",
    "\n",
    "This leads us to the following linear model:\n",
    "\n",
    "$$y_i = b_0 + b_1 \\times x_{i1} +  b_2 x_{i2} = y,\\ i=1,...,n$$\n",
    "\n",
    "#### Data preparation\n",
    "The column 'Sales' from our DataFrame define the response vector $y$ and the columns 'TV' and 'Radio' define the predictor matrix $X$. For our algorithm, we additionally need a column of ones as the column of the matrix $X$ that is associated with $b_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Ones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Radio  Ones\n",
       "1  230.1   37.8     1\n",
       "2   44.5   39.3     1\n",
       "3   17.2   45.9     1\n",
       "4  151.5   41.3     1\n",
       "5  180.8   10.8     1"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.Sales\n",
    "X = df[['TV','Radio']].copy()\n",
    "X['Ones'] = np.ones(len(df))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier indexing, let us use the underlying numpy arrays instead of working with the slices from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = y.values\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Decent\n",
    "\n",
    "Since the elements of the gradient are a function of $b$, we must select an initial point $b_0$ to obtain a gradient with constant elements. This gradient points in the direction of steepest descent at the given point $b_0$. We can therefore use this gradient to select a new point $b_1$ that is closer to the optimum than $b_0$. If we do this iteratively, we will obtain a sequence of $b$'s which will get closer and closer to the optimum.\n",
    "\n",
    "A problem with gradient descent is that, as we approach the optimum the gradient may point towards the optimal $b$, but its length is larger than the step that needs to be take to get to $b^*$. We therefore work our way towards the optimum in smaller steps. The size of these steps, $\\alpha$, also referred to as stepsize, is an important parameter in the gradient descent method, and subject to a lot of research.\n",
    "\n",
    "1. Choose initial guess $\\hat{b}_0$, stepsize $\\alpha$ \n",
    "2. <b>for</b> k = 0, 1, 2, ... <b>do</b>\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\nabla Q_k(b_k) = -\\frac{1}{n} \\sum_{i=1}^{n}(\\hat{b}_k' x_i - y_i)x_i$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat{b}_{k+1} = \\hat{b}_k + \\alpha \\nabla Q_k(b_k)$ \n",
    "\n",
    "For our sales advertising model, we select $b_0=(0,0,0)$ and $\\alpha=0.00001$ and compute a sequence of 100 $b$'s, after which the algorithm has hopefully converged towards a near-optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.zeros(3)\n",
    "alpha = 0.00005\n",
    "n = len(df)\n",
    "for j in range(100):\n",
    "    grad = np.zeros(3)\n",
    "    for i in range(n):\n",
    "        grad += (y[i]-b.dot(X[i]))*-X[i]/n\n",
    "    b = b - alpha*grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the mean of the (residual) sum of squares (also known as the mean squared error or MSE), the $R^2$, as well as the vector of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.7591935940908296, 0.82429156688411653, array([ 0.0599,  0.1939,  0.0057]))"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss =  sum((y[i]-b.dot(X[i]))**2 for i in range(n))\n",
    "tss = sum((y[i]-y.mean())**2 for i in range(n))\n",
    "r_squared = 1.-rss/tss\n",
    "mse = rss/n\n",
    "mse, r_squared, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "We compute the partial derivative by collecting the slopes associated with each observation $x_i$. An alternative to taking the sum is to keep updating the gradient while collecting these terms. If we additionally randomize the sequence in which we collect the terms, we arrive at an algorithm referred to as stochastic gradient decent.\n",
    "\n",
    "Instead of computing the full gradient, we update the gradient after having obtained the fraction of the slope that corresponds to a randomly drawn observation $x_i$.\n",
    "\n",
    "1. Choose initial guess $\\hat{b}_0$, stepsize $\\alpha$ \n",
    "2. <b>for</b> k = 0, 1, 2, ... <b>do</b>\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Draw randomly $i \\sim U(1,n)$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\nabla Q_k = - (\\hat{b}_k' x_i - y_i)x_i$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat{b}_{k+1} = \\hat{b}_k + \\alpha \\nabla Q_k$\n",
    "\n",
    "For our implementation, we now replace the inner loop and instead compute the gradient for only one observation. The index of that observation is draw uniformly from the range $[0,n]$.\n",
    "\n",
    "Let us also select a vector of stepsizes $\\alpha=(0.00001,0.0001,0.05)$. Since parameters are associated with  features of different magintudes, better results can be obtained by selecting parameter-specific stepsizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "b = np.zeros(3)\n",
    "alpha = [0.00001,0.0001,0.05]\n",
    "n = len(df)\n",
    "for j in range(100):\n",
    "    i = random.randint(0,n-1)\n",
    "    grad = (y[i]-b.dot(X[i]))*-X[i]\n",
    "    b = b - alpha*grad  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again look at MSE, $R^2$ and vector $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.8397267616078925, 0.89515788128919682, array([ 0.0445,  0.1906,  2.8471]))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss =  sum((y[i]-b.dot(X[i]))**2 for i in range(n))\n",
    "tss = sum((y[i]-y.mean())**2 for i in range(n))\n",
    "r_squared = 1.-rss/tss\n",
    "mse = rss/n\n",
    "mse, r_squared, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the stochastic version of the algorithm can obtain a higher quality solution with less work. In this case, this is due to the different choice of stepsizes, which highlights its importance for this class of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "Sicne gradient descent is highly dependent on the selected stepsize, it should ideally  be chosen for each dimension separately. There are numerous approaches to select stepsizes, one of which is AdaGrad.\n",
    "\n",
    "AdaGrad is a simple extension of stochastic gradient descent to select a stepsize based on the magnitude of the gradient in each dimension.\n",
    "\n",
    "In addition to computing the gradient, AdaGrad stores the sum of squared gradients. With $G= \\nabla Q(b)$, we define the sum of previous squared gradients\n",
    "as \n",
    "\n",
    "$$G_k=\\left(\\sum_{j=0}^k \\nabla Q(b)_{1j}^2,...,\\sum_{j=0}^k\\nabla Q(b)_{1j}^2\\right)$$\n",
    "\n",
    "The stepsize now becomes the root of the sum of previous squared gradients:\n",
    "\n",
    "$$\\alpha_k=\\left(\\sqrt{G_{k1}},...,\\sqrt{G_{kp}}\\right)$$\n",
    "\n",
    "The stochastic gradient descent algorithm using AdaGrad then becomes,\n",
    "\n",
    "1. Choose initial guess $\\hat{b}_0$, $G_0=0$ \n",
    "2. <b>for</b> k = 0, 1, 2, ... <b>do</b>\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Draw randomly $i \\sim U(1,n)$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\nabla Q_k = - (\\hat{b}_k' x_i - y_i)x_i$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $G_{k+1} = G_k+((\\hat{b}_k' x_i - y_i)x_i)^2$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\hat{b}_{k+1} = \\hat{b}_k + \\frac{1}{\\sqrt{G_{k+1}}} \\nabla Q_k$\n",
    "\n",
    "This algorithm is extremely easy to implement, it is theoretically sound and has been proven quite successful empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Solution\n",
    "\n",
    "As mentioned above, the optimal vector $b^*$ can be obtained analytically by setting the partial derivatives jointly to zero, which entails solution of a system of equations, and as such requires matrix inversion. The method is typically referred to as ordinary least squares or OLS.\n",
    "\n",
    "Let us skip the mathematical derivation of OLS and briefly present the OLS solution using a library called [Statsmodels](http://statsmodels.sourceforge.net). Statsmodels brings classic statistical analysis to Python. Running a simple regression on our model is quite simply, as Statsmodels gets along well with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[['TV','Radio']]\n",
    "X = sm.add_constant(X)\n",
    "reg = sm.OLS(y,X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a summary of the result of the regression which shows us a number of quality of fit measures, information critera, test statistics, as well as the parameters themselves.\n",
    "\n",
    "Comparing the $R^2$ with the one from above, we can see that with the right step size, our gradient descent algorithm is near optimal, with much less work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   859.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 12 Oct 2016</td> <th>  Prob (F-statistic):</th> <td>4.83e-98</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:02:46</td>     <th>  Log-Likelihood:    </th> <td> -386.20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   778.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   197</td>      <th>  BIC:               </th> <td>   788.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    2.9211</td> <td>    0.294</td> <td>    9.919</td> <td> 0.000</td> <td>    2.340     3.502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TV</th>    <td>    0.0458</td> <td>    0.001</td> <td>   32.909</td> <td> 0.000</td> <td>    0.043     0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Radio</th> <td>    0.1880</td> <td>    0.008</td> <td>   23.382</td> <td> 0.000</td> <td>    0.172     0.204</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>60.022</td> <th>  Durbin-Watson:     </th> <td>   2.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 148.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.323</td> <th>  Prob(JB):          </th> <td>5.19e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.292</td> <th>  Cond. No.          </th> <td>    425.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.897\n",
       "Model:                            OLS   Adj. R-squared:                  0.896\n",
       "Method:                 Least Squares   F-statistic:                     859.6\n",
       "Date:                Wed, 12 Oct 2016   Prob (F-statistic):           4.83e-98\n",
       "Time:                        11:02:46   Log-Likelihood:                -386.20\n",
       "No. Observations:                 200   AIC:                             778.4\n",
       "Df Residuals:                     197   BIC:                             788.3\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "const          2.9211      0.294      9.919      0.000         2.340     3.502\n",
       "TV             0.0458      0.001     32.909      0.000         0.043     0.048\n",
       "Radio          0.1880      0.008     23.382      0.000         0.172     0.204\n",
       "==============================================================================\n",
       "Omnibus:                       60.022   Durbin-Watson:                   2.081\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              148.679\n",
       "Skew:                          -1.323   Prob(JB):                     5.19e-33\n",
       "Kurtosis:                       6.292   Cond. No.                         425.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Selection\n",
    "\n",
    "Apart from the algorithms themselves, another important aspect in machine learning is variable selection. In linear models, often, not only the predictors themselves are used, but also combinations of predictors,\n",
    "\n",
    "$$\\text{sales} = b_0 + b_1 \\times \\text{TV} +  b_2 \\times \\text{radio} +  b_3 \\times \\text{TV} \\times \\text{radio} $$\n",
    "\n",
    "Let us add this interaction to our model by appending a new column to our DataFrame that contains the product of 'TV' and 'Radio'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df[['TV','Radio']].copy()\n",
    "X['TVxRadio'] = df.TV*df.Radio\n",
    "X = sm.add_constant(X)\n",
    "reg = sm.OLS(y,X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to looking at quality of fit measure, we can also visually inspect the model fit by plotting the residuals agains the individal parameters. This may help us to identify whether there is some non-linear behavious in the parameters that is not captured by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7AAAAFHCAYAAACYtaBKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe2d2cf650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X['resid'] = reg.resid\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(16,5))\n",
    "X.plot(kind='scatter', x='TV', y='resid', ax=ax1)\n",
    "X.plot(kind='scatter', x='Radio', y='resid', ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there seems to be a non-linear relationship between 'TV' and the residuals. Especially, it appears that our prediction is too large for low and high values of 'TV' and too low for intermediate values. Let us therefore add polynomials of degree $d$ to our model.\n",
    "\n",
    "$$\\text{sales} = b_0 + b_1 \\times \\text{TV} +  b_2 \\times \\text{radio} +  b_3 \\times \\text{TV} \\times \\text{radio} + b_4\\text{TV}^2 + ... + b_{d+2} \\text{TV}^d $$\n",
    "\n",
    "The question is now, how many degrees our polynomial should have and which terms to select. To automatically select the right number of polynomials of given degree $d$, we are going to use the Aikaike information criterion (AIC), \n",
    "\n",
    "$$AIC = 2p + n \\ln \\left( \\frac{RSS}{n}\\right)$$\n",
    "\n",
    "The AIC measures the quality of our model relative to other candiate models by balancing model complexity against the goodness-of-fit (see: bias variance trade-off). In our linear model, model complexity scales with the number of parameters in our model, whereas goodness-of-fit is measured through the MSE.\n",
    "\n",
    "To find the model with the best fit, we iteratively add a polynomial of higher degree and print the lowest AIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree: 1,  aic: 548.277841695\n",
      "degree: 2,  aic: 383.718517832\n",
      "degree: 3,  aic: 298.309012955\n",
      "degree: 4,  aic: 247.326443247\n",
      "degree: 5,  aic: 217.530197631\n",
      "degree: 6,  aic: 254.707290776\n",
      "degree: 7,  aic: 761.732525533\n",
      "degree: 8,  aic: 1210.62389444\n",
      "degree: 9,  aic: 1253.98282773\n",
      "degree: 10,  aic: 1290.34609384\n"
     ]
    }
   ],
   "source": [
    "X = df[['TV','Radio']].copy()\n",
    "X['TVxRadio'] = df.TV*df.Radio\n",
    "X = sm.add_constant(X)\n",
    "reg = sm.OLS(y,X).fit()\n",
    "print \"degree: %d, \"%1,\"aic:\",reg.aic\n",
    "for degree in range(2,11):\n",
    "    X['TV^%d'%degree] = X.TV.pow(degree)\n",
    "    reg = sm.OLS(y,X).fit()\n",
    "    print \"degree: %d, \"%degree,\"aic:\",reg.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Instead of using the AIC to select a model, we can use also cross validation. While cross validation is computationally more expensive, it is more generally applicable, and we will revisit it throughout this course for parameter / model selection.\n",
    "\n",
    "Since Statsmodels does not provide functions for cross validation, let us switch to [scikit-learn](http://scikit-learn.org/stable/), the machine learning library in Python. First, we must import the necessary modules, LinearRegression and KFold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we separate predictors and response into separate arrays and create a  [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object. The  object provides the functions [fit(X,y)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit), [predict(X)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict), and [score(X,y)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score) ($=R^2$) which are provided by all of scikit's predcitor objects, as we will see later in this course.\n",
    "\n",
    "Let us compute the *in-sample* fit of our model by fitting a model to our data and then computing the $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89719426108289557"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['TV','Radio']].copy()\n",
    "y = df.Sales\n",
    "reg = LinearRegression()\n",
    "reg.fit(X,y)\n",
    "reg.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cross validation, we will use the object [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html). All it does is provide an iterator over the k folds, each consisting of a pair of a set of indices, one for the test and one for the training set. These indices can then be used to select the elements that correspond to a set of observations from the complete data set.\n",
    "\n",
    "For each fold, we will perform the regression on the training set and then compute the $R^2$ for the test set. We then sum up all the $R^2$ from each fold and compute its average, which will serve as our *out-of-sample* fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87772379750156648"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score = 0.\n",
    "for test,train in KFold(n, n_folds=5):\n",
    "    X_test, X_train =  X.values[test], X.values[train] #select data using internal numpy array\n",
    "    y_test, y_train =  y.iloc[test], y.iloc[train] #select data using Pandas' iloc function\n",
    "    reg.fit(X_train,y_train)\n",
    "    avg_score += reg.score(X_test,y_test)/5.\n",
    "avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Implement AdaGrad and use it to fit the first advertising sales model. What do you observe when comparing it to stochastic gradient descent or the analytical solution from Statsmodels?\n",
    "2. Wrap stochastic gradient descent (AdaGrad) into a separate function that requires X and y as input arguments (optional: iterations, default = 10n) and returns the parameter vector b. This function should be used from now on two fit models.\n",
    "3. Run gradient descent prediction of Sales = b_0 + b_1*TV and plot the prediction against the original data using Matplotlib.\n",
    "4. Use cross validation to select the right order of the polynomial. Automatically stop when the best $R^2$ is found. What do you observe when comparing it to the solution using the AIC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
